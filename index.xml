<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Review Him Blog</title>
    <link>https://review-him.github.io/</link>
    <description>Recent content on Review Him Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 28 Jan 2024 20:11:01 +0700</lastBuildDate><atom:link href="https://review-him.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Summarunner architecture</title>
      <link>https://review-him.github.io/posts/summarunner_architecture/</link>
      <pubDate>Sun, 28 Jan 2024 20:11:01 +0700</pubDate>
      
      <guid>https://review-him.github.io/posts/summarunner_architecture/</guid>
      <description>О способах решения задачи саммаризации Для решения задачи саммаризации используются техники, основанные на двух подходах: extractive и abstractive
Extractive подходы Выбирают важные имеющиеся предложения или отрывки из документа
Abstractive подходы Могут перефразировать документ, назвав важную информацию из него другими словами
 Модель SummaRuNNer реализует именно Extractive подход, основанный на рекуррентных нейросетях. А подход для ее обучения позволяет extractive модели быть натренированной на abstractive summaries
Архитектура модели Данная модель решает проблему саммаризации следующим образом: решается проблема бинарной классификации для каждого предложения в документе (брать предложение в саммари или не брать), учитывая классификацию предсказанную моделью для предыдущих предложений от текущего.</description>
    </item>
    
    <item>
      <title>GRU - Gated Recurrent Unit</title>
      <link>https://review-him.github.io/posts/gru_model/</link>
      <pubDate>Mon, 15 Jan 2024 22:58:45 +0700</pubDate>
      
      <guid>https://review-him.github.io/posts/gru_model/</guid>
      <description>Что такое GRU Расшифровка этой аббревиатуры выглядит так - The Gated Recurrent Unit. Это усовершенствованная архитектура нейронной сети, которая перекрывает некоторые недостатки таких архитектур, как RNN и LSTM. В отличие от RNN, эта архитектура может запоминать более далекие зависимости, а также в этой архитектуре в какой-то степени решена проблема затухания и взрыва градиента. А в отличие от LSTM, эта архитектура имеет меньшее количество параметров, что повышает скорость ее обучения, но не сильно ухудшает качество предсказаний модели.</description>
    </item>
    
    <item>
      <title>RNN architecture</title>
      <link>https://review-him.github.io/posts/rnn_model/</link>
      <pubDate>Sun, 14 Jan 2024 18:44:47 +0700</pubDate>
      
      <guid>https://review-him.github.io/posts/rnn_model/</guid>
      <description>Где RNN используются Технология рекуррентных нейронных сетей используется как основа в state of the art решениях в области языкового моделирования и генерации текста, распознавания речи, генерации описания изображений или видео тагинга.
Такой тип архитектуры нейронных сетей используется преимущественно в задачах определения паттернов в последовательности данных. С помощью RNN можно обрабатывать такие данные как рукописный текст, геномы, текстовые или числовые временные серии (например котировки акций или показания датчиков). А также на вход такой модели можно подавать и части изображения для решения каких либо специфичных задач, как например описание изображения.</description>
    </item>
    
    <item>
      <title>Eigenvalues &amp; Eigenvectors</title>
      <link>https://review-him.github.io/posts/eigenvalues_and_eigenvectors/</link>
      <pubDate>Fri, 05 Jan 2024 22:00:00 +0000</pubDate>
      
      <guid>https://review-him.github.io/posts/eigenvalues_and_eigenvectors/</guid>
      <description># Eigenvalues &amp;amp; Eigenvectors : Data Science Basics
 Что такое eigenvalues и eigenvectors Eigenvalues и Eigenvectors Предположим, что у нас есть квадратная матрица A. Тогда умножив ее на какой-либо вектор $x$, где $x \neq 0$, получим этот же вектор $x$, умноженный на какое-либо значение $\lambda$, где $\lambda \in R$ (принадлежит области вещественных чисел). Все это описывается такой формулой: $Ax = \lambda x$. В этом случае $\lambda$ является собственным значением матрицы $A$ (Eigenvalue), а $x$ - собственным вектором (Eigenvector).</description>
    </item>
    
  </channel>
</rss>

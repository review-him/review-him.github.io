<!DOCTYPE html>
<html><head>
<meta charset="utf-8"/>
<meta name='viewport' content='width=device-width, initial-scale=1'>
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Roboto&display=swap" rel="stylesheet"> 
<link href="/css/style.css" rel="stylesheet" type="text/css" media="all">


<title>Review Him Blog | Loss functions</title>
</head>
<body>
		 <div class="max-w-{1400px}">

<div class="py-6 pl-4 lg:pl-8 mb-5">
<a href="/" class="text-5xl block mb-6">Review Him Blog </a>
<span class="text-xl">  </span>
</div>

<nav class="flex flex-row gap-8 pl-4 lg:pl-8">
        

                

                
                <div class="pb-3 border-b-white border-b-4 text-lg hover:border-b-red-700">

                                <a  href="/"> Home</a>
                        
                </div>
                
        

                

                
                <div class="pb-3 border-b-white border-b-4 text-lg hover:border-b-red-700">

                                <a  href="/posts/"> Blog</a>
                        
                </div>
                
        

                

                
                <div class="pb-3 border-b-white border-b-4 text-lg hover:border-b-red-700">

                                <a  href="/about/"> About</a>
                        
                </div>
                
        
</nav>
<hr><div class="grid grid-cols-12 gap-8 mx-4 lg:mx-8 mt-10">
			<div class="col-span-12 lg:col-span-8">


<h1 class="text-2xl font-bold mb-2">Loss functions</h1>


<div class="flex flex-row gap-2 mb-2">

<div>
	<datetime class="text-zinc-500">2024-02-02</datetime></div>


</div>

<div class="single">

<p>Функция потерь (Loss function) - функция, которая говорит, насколько предсказания модели далеки от правды. С помощью функции потерь, можно подстраивать параметры модели таким образом, чтобы минимизировать значения этой функции, тем самым увеличивая точность предсказаний модели. А в статистике функции потерь используются для оценки подобранных параметров функций.</p>
<p>Далее я разберу распространенные функции потерь в задачах классификации и регрессии, а затем приведу примеры их расчета и использования на практике.</p>
<h1 id="функции-потерь-для-регрессии">Функции потерь для регрессии</h1>
<h2 id="mean-square-error--quadratic-loss--l2-loss">Mean Square Error / Quadratic Loss / L2 Loss</h2>
<p>Самая часто используемая функция потерь в задачах регрессии - это MSE loss.
Формула этой функции выглядит следующим образом:
$$
\text{MSE} = \dfrac{1}{n} \sum_{i=1}^n (\hat{Y}_i - Y_i)^2
$$
Эта функция потерь примечательна тем, что возводит отклонение от правильного ответа в квадрат, тем самым сильно штрафуя за большие отклонения. Из-за этого функция чувствительна к выбросам.
<img src="/images/loss_03.png" alt="график mse"></p>
<h2 id="mean-absolute-error--l1-loss">Mean Absolute Error / L1 Loss</h2>
<p>MAE - это вторая по частоте использования для задач регрессии.
Ее формула выглядит похоже на предыдущую, с некоторыми отличиями (квадрат поменялся на модуль):
$$
\text{MAE} = \dfrac{1}{n} \sum_{i=1}^n |\hat{Y}_i - Y_i|
$$
Эта функция меньше подвержена влиянию выбросов, в сравнении с MSE, поэтому если выбросов в данных много, следует воспользоваться ей.
<img src="/images/loss_04.png" alt="график mae"></p>
<h2 id="huber-loss--smooth-mean-absolute-error">Huber Loss / Smooth Mean Absolute Error</h2>
<p>Эта функция потерь объединяет в себе две предыдущие, влияние которых зависит от дополнительного параметра $\sigma$:
$$
L_\sigma(Y, \hat{Y})=
\begin{cases}
\dfrac{1}{2}(Y-\hat{Y})^2&amp;,&amp; \text{for } |Y - \hat{Y}| \le \sigma \newline
\sigma \left(|Y - \hat{Y}| - \dfrac{1}{2} \sigma\right) &amp;,&amp;\text{ otherwise }
\end{cases}
$$
Эта функция более устойчива к выбросам, но следует правильно подбирать параметр $\sigma$.
<img src="/images/loss_05.png" alt="график huber loss"></p>
<h2 id="log-cosh-loss">Log-cosh Loss</h2>
<p>Эта функция потерь представляет собой логарифм гиперболического косинуса ошибки предсказаний модели. Формула этой функции выглядит следующим образом (В английских обозначениях функция гиперболического косинуса выглядит как $cosh$, но в русских источниках ее обозначают как $ch$ - так написано в википедии:D):
$$
L(Y, \hat{Y}) = \sum_{i=1}^{n} log(cosh(\hat{Y_i} - Y_i))
$$</p>
<p><img src="/images/loss_06.png" alt="график logcosh">
Так как я изучаю концепции в ML, а также вспоминаю высшую математику, по мере встречаемости терминов и функций в формулах, то у меня возник вопрос, а чем функция $cosh$ отличается от обычного $cos$.
Я загуглил картинки графиков этих функций, чтобы понять различия и вот что выдал мне гугол:
<img src="/images/loss_01.png" alt="график cosh">
Оказывается, гиперболические функции называются так не спроста. Они означают, что берется обычная тригонометрическая функция, но определяется она через гиперболу, а не через окружность.
Формула гиперболического косинуса выглядит следующим образом:
$$
\begin{array}{l}
cosh(x) &amp;=&amp; \frac{e^x + e^{-x}}{2} = \newline
&amp;=&amp; \frac{e^{2x} + 1}{2e^x} = \newline
&amp;=&amp; \frac{1 + e^{-2x}}{2e^{-x}}
\end{array}
$$
Функции потерь, благодаря своему виду ($log(cosh(x))$), для маленьких ошибок вернет примерно тоже что и функция $(x^2) / 2$, а для больших значений ошибки вернет $abs(x) - log(2)$. Получается, что эта функция похожа на MSE Loss, кроме того, что она будет не так сильна подвержена влиянию больших значений ошибки.</p>
<h2 id="quantile-loss">Quantile Loss</h2>
<p>Название этой функции подсказывает нам, что она используется для обучения модели, которая предсказывает значение квантиля.</p>
<h2 id="что-такое-квантиль">Что такое квантиль</h2>
<p>Квантиль - это значение, которое делит заданный набор чисел таким образом что $\alpha * 100%$ чисел - меньше, чем этот квантиль - значение, а $(1 - \alpha) * 100%$ чисел больше, чем этот квантиль. Такое определение не просто понять, поэтому на этой картинке есть пример того, как это работает и что из этого квантиль.</p>
<p>Для примера, возьмем рандомный список отсортированных чисел в порядке возрастания. Поделим этот список на 4 части таким образом, чтобы каждая следующая точка отделяла на 25% больше элементов этого списка. Еще один важный момент - точка должна быть выбрана из уже имеющихся значений в нашем списке. Вот так должно это все выглядеть:
<img src="/images/loss_02.png" alt="визуализация квантилей">
На этой картинке мы видим, что первый квантиль равен 10, потому что он отделяет 25% данных слева и 75% данных справа. Следующий квантиль будет равен 19, потому что он отделяет 50% данных слева и 50% данных справа, а третий квантиль будет равен 26, по той же логике.</p>
<h2 id="как-работает-quantile-loss">Как работает quantile loss</h2>
<p>Предположим у нас есть какая-то модель, которая как раз и предсказывает значение какого-нибудь квантиля.
Если мы намерены предсказывать третий квантиль, то это означает, что в 75% случаев ошибки наших предсказаний должны быть отрицательными, а в остальных 25% случаев положительными.
В этом мы убедимся посмотрев на формулу функции потерь:
$$
L_\sigma (Y, \hat{Y}) =
\begin{cases}
\alpha(Y - \hat{Y}) &amp;, \hat{Y} \le Y &amp;\newline
(1-\alpha)(\hat{Y} - Y) &amp;, \hat{Y} &gt; Y &amp;
\end{cases}
$$
Как видим в формуле присутствует параметр $\alpha$, который означает персентиль, который мы хотим предсказать. От этого параметра будет зависить то, насколько сильно мы будем штрафовать модель за предсказания до или после персентиля или одинаково в обе стороны. Если мы рассмотрим эту формулу с параметром $\alpha = 4$, то мы увидим, что модель в таком случае будет гораздо сильнее штрафовать если мы предсказали значение меньше, чем исходное значение. А предсказания больше исходного значения будут штрафоваться меньше.
<img src="/images/loss_07.png" alt="график quantile loss"></p>
<h1 id="функции-потерь-для-классификации">Функции потерь для классификации</h1>
<h2 id="binary-cross-entropy-loss--log-loss">Binary Cross-Entropy Loss / Log Loss</h2>
<p>Самая распространенная функция потерь для решения проблемы классификации.
Суть этой функции потерь в том, что ее значение уменьшается по мере того, как модель предсказывает значение приближенное к истинному.
Используется эта функция, когда модель возвращает значения между 0 и 1, а количество классов модели = 2. Это и означает бинарность в названии этой функции.</p>
<p>$$
L(Y, \hat{Y}) = -\frac{1}{n} \sum_{i=1}^{n} (Y_i log(\hat{Y_i}) + (1 - Y_i) log(1-\hat{Y_i}))
$$
<img src="/images/loss_08.png" alt="график bce loss">
Для мультиклассовой классификации, когда количество классов больше 2, используется следующая функция:
$$
L(Y, \hat{Y}) = -\frac{1}{n} \sum_{i=1}^n Y_i log(\hat{Y}_i)
$$</p>
<h2 id="hinge-loss">Hinge Loss</h2>
<p>Вторая распространенная функция потерь для решения задач классификации. Ее применяют для SVM моделей.
Ее формула выглядит следующим образом:
$$
L(Y, \hat{Y}) = max(0, 1 - Y * \hat{Y})
$$</p>
<p>Эта функция потерь штрафует неправильные предсказания, а также неуверенные правильные предсказания.
При использовании этой функции для классификации в SVM используются истинные значения классов -1 и 1, поэтому для использования этой функции нужно убедиться, что истинные значения в данных соответствуют -1 и 1
<img src="/images/loss_09.png" alt="график hinge loss"></p>
<h1 id="практика">Практика</h1>
<p>В качестве практики я собираюсь взять пару функций для регрессии и одну для классификации.</p>
<h2 id="начнем-с-функции-log-loss-для-классификации">Начнем с функции Log Loss для классификации</h2>
<p>Для упрощения использую библиотеку pytorch. Код самой функции выглядит следующим образом:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">log_loss</span>(y_true, y_pred):
    choosen_probas <span style="color:#f92672">=</span> y_pred<span style="color:#f92672">.</span>gather(<span style="color:#ae81ff">1</span>, y_true<span style="color:#f92672">.</span>view(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>))
    log_probas <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>log10(choosen_probas)
    <span style="color:#66d9ef">return</span> <span style="color:#f92672">-</span>torch<span style="color:#f92672">.</span>mean(y_true <span style="color:#f92672">*</span> log_probas)
</code></pre></div><p>В коде этой функции нет ничего сверхъестественного, кроме функции gather, которую мы применили к y_pred. В нашем случае данные в переменной y_true выглядят следующим образом: <code>[0, 1, 2, 1, ...]</code>, то есть означают номера классов, которые мы и пытаемся предсказать, а значения в переменной y_pred выглядят <code>[[0.5, 0.25, 0.25], [...], [...], [...], ...]</code>, то есть у нас для каждого входящего массива признаков на выходе 3 нейрона возвращают вероятности для каждого класса.
Поэтому в данном случае функция .gather() принимает параметр в доль какой оси выполнять соответствие индексов массива y_true, элементам массива y_pred.
Если еще проще, мы видим, что первый элемент массива y_true у нас 0, поэтому мы берем из первого массива y_pred значение вероятности 0, которую нам вернул нейрон модели, для следующего элемента из массива y_true, мы берем в следующем массиве y_pred значение вероятности с индексом 1 и так далее.</p>
<p>Кстати, в pytorch уже есть готовая реализация этой функции CrossEntropyLoss(), которая находится в модуле torch.nn, работает по батчам, а также имеет несколько дополнительных параметров.</p>
<h2 id="mean-square-error">Mean Square Error</h2>
<p>Без лишних слов оставляю здесь код функции. Функция mean, а также операция вычитания массивов с предсказаниями и правильными ответами, делаются из расчета на то, что их типы - это тензоры из pytorch или массивы из numpy.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">mse_loss</span>(y_pred, y_true):
    <span style="color:#66d9ef">return</span> ((y_pred <span style="color:#f92672">-</span> y_true) <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>)<span style="color:#f92672">.</span>mean()
</code></pre></div><h2 id="log-cosh-loss-1">Log-cosh Loss</h2>
<p>В pytorch код функции выглядел бы так.
При работе с большими отклонениями могут возникать некоторые трудности с подсчетами, поскольку e в большой степени затрудняет эти самые подсчеты.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">log_cosh</span>(y_pred, y_true):
    dif <span style="color:#f92672">=</span> y_pred <span style="color:#f92672">-</span> y_true
    cosh <span style="color:#f92672">=</span> (torch<span style="color:#f92672">.</span>exp(<span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> dif) <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">/</span> (<span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> torch<span style="color:#f92672">.</span>exp(dif))
    <span style="color:#66d9ef">return</span> torch<span style="color:#f92672">.</span>log(cosh)<span style="color:#f92672">.</span>mean()
</code></pre></div><h1 id="источники">Источники</h1>
<blockquote>
<p><a href="https://www.youtube.com/watch?v=eKIX8F6RP-g">https://www.youtube.com/watch?v=eKIX8F6RP-g</a></p>
<p><a href="https://wiki.loginom.ru/articles/loss-function.html">https://wiki.loginom.ru/articles/loss-function.html</a></p>
<p><a href="https://builtin.com/machine-learning/common-loss-functions">https://builtin.com/machine-learning/common-loss-functions</a></p>
<p><a href="https://en.wikipedia.org/wiki/Huber_loss">https://en.wikipedia.org/wiki/Huber_loss</a></p>
<p><a href="https://en.wikipedia.org/wiki/Hyperbolic_functions">https://en.wikipedia.org/wiki/Hyperbolic_functions</a></p>
</blockquote>


</div>




<script type="text/javascript"
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ 
    tex2jax: {
      inlineMath: [["$","$"],["\\(","\\)"]],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
    }
  })
</script>


			</div>

			<aside class="lg:col-span-4 col-span-12">
					<h3 class="text-lg mb-2 text-black font-medium "> Explore topics by </h3>
					<ul class="text-cyan-900">
						<li><a href="/categories/" class="hover:underline"> Categories </a></li>
						<li><a href="/tags/" class="hover:underline"> Tags </a></li>
					</ul>


					
					
					<h3 class="text-lg mb-2 mt-3 text-black font-medium ">Featured Posts</h3>
						<ul class="text-cyan-900">
						
					</ul>

					

					
					

					
					
					


			</aside>

</div>

        

		</div><div class="bg-slate-100 py-10 px-4 lg:px-8 mt-20">
<p>Copyright (c) 2024 </p>
</div></body>
</html>



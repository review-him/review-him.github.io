<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Review Him Blog</title>
    <link>https://review-him.github.io/posts/</link>
    <description>Recent content in Posts on Review Him Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 13 Feb 2024 14:56:14 +0700</lastBuildDate><atom:link href="https://review-him.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Threshold moving</title>
      <link>https://review-him.github.io/posts/threshold_moving/</link>
      <pubDate>Tue, 13 Feb 2024 14:56:14 +0700</pubDate>
      
      <guid>https://review-him.github.io/posts/threshold_moving/</guid>
      <description>Зачем? Когда у нас примерно одинаковое количество примеров для разных классов, не возникает такой проблемы, как выбор трэшхолда, поскольку тут интуитивно понятно, что значение вероятности больше 0.5 означает позитивное предсказание, а меньше означает негативное.
Но что делать, когда у нас есть перевес в данных в какую-либо сторону? Или цена ошибки в каком либо классе больше, чем в другом? Тогда и нужно решать задачу подбора трэшхолда. Для решения этой задачи существует не один подход, а в этой статье я рассмотрю лишь часть из них.</description>
    </item>
    
    <item>
      <title>Loss functions</title>
      <link>https://review-him.github.io/posts/loss_functions/</link>
      <pubDate>Fri, 02 Feb 2024 21:26:28 +0700</pubDate>
      
      <guid>https://review-him.github.io/posts/loss_functions/</guid>
      <description>Функция потерь (Loss function) - функция, которая говорит, насколько предсказания модели далеки от правды. С помощью функции потерь, можно подстраивать параметры модели таким образом, чтобы минимизировать значения этой функции, тем самым увеличивая точность предсказаний модели. А в статистике функции потерь используются для оценки подобранных параметров функций.
Далее я разберу распространенные функции потерь в задачах классификации и регрессии, а затем приведу примеры их расчета и использования на практике.
Функции потерь для регрессии Mean Square Error / Quadratic Loss / L2 Loss Самая часто используемая функция потерь в задачах регрессии - это MSE loss.</description>
    </item>
    
    <item>
      <title>Summarunner architecture</title>
      <link>https://review-him.github.io/posts/summarunner_architecture/</link>
      <pubDate>Sun, 28 Jan 2024 20:11:01 +0700</pubDate>
      
      <guid>https://review-him.github.io/posts/summarunner_architecture/</guid>
      <description>О способах решения задачи саммаризации Для решения задачи саммаризации используются техники, основанные на двух подходах: extractive и abstractive
Extractive подходы Выбирают важные имеющиеся предложения или отрывки из документа
Abstractive подходы Могут перефразировать документ, назвав важную информацию из него другими словами
 Модель SummaRuNNer реализует именно Extractive подход, основанный на рекуррентных нейросетях. А подход для ее обучения позволяет extractive модели быть натренированной на abstractive summaries.
Архитектура модели Данная модель решает проблему саммаризации следующим образом: решается проблема бинарной классификации для каждого предложения в документе (брать предложение в саммари или не брать), учитывая классификацию предсказанную моделью для предыдущих предложений от текущего.</description>
    </item>
    
    <item>
      <title>GRU - Gated Recurrent Unit</title>
      <link>https://review-him.github.io/posts/gru_model/</link>
      <pubDate>Mon, 15 Jan 2024 22:58:45 +0700</pubDate>
      
      <guid>https://review-him.github.io/posts/gru_model/</guid>
      <description>Что такое GRU Расшифровка этой аббревиатуры выглядит так - The Gated Recurrent Unit. Это усовершенствованная архитектура нейронной сети, которая перекрывает некоторые недостатки таких архитектур, как RNN и LSTM. В отличие от RNN, эта архитектура может запоминать более далекие зависимости, а также в этой архитектуре в какой-то степени решена проблема затухания и взрыва градиента. А в отличие от LSTM, эта архитектура имеет меньшее количество параметров, что повышает скорость ее обучения, но не сильно ухудшает качество предсказаний модели.</description>
    </item>
    
    <item>
      <title>RNN architecture</title>
      <link>https://review-him.github.io/posts/rnn_model/</link>
      <pubDate>Sun, 14 Jan 2024 18:44:47 +0700</pubDate>
      
      <guid>https://review-him.github.io/posts/rnn_model/</guid>
      <description>Где RNN используются Технология рекуррентных нейронных сетей используется как основа в state of the art решениях в области языкового моделирования и генерации текста, распознавания речи, генерации описания изображений или видео тагинга.
Такой тип архитектуры нейронных сетей используется преимущественно в задачах определения паттернов в последовательности данных. С помощью RNN можно обрабатывать такие данные как рукописный текст, геномы, текстовые или числовые временные серии (например котировки акций или показания датчиков). А также на вход такой модели можно подавать и части изображения для решения каких либо специфичных задач, как например описание изображения.</description>
    </item>
    
    <item>
      <title>Eigenvalues &amp; Eigenvectors</title>
      <link>https://review-him.github.io/posts/eigenvalues_and_eigenvectors/</link>
      <pubDate>Fri, 05 Jan 2024 22:00:00 +0000</pubDate>
      
      <guid>https://review-him.github.io/posts/eigenvalues_and_eigenvectors/</guid>
      <description># Eigenvalues &amp;amp; Eigenvectors : Data Science Basics
 Что такое eigenvalues и eigenvectors Eigenvalues и Eigenvectors Предположим, что у нас есть квадратная матрица A. Тогда умножив ее на какой-либо вектор $x$, где $x \neq 0$, получим этот же вектор $x$, умноженный на какое-либо значение $\lambda$, где $\lambda \in R$ (принадлежит области вещественных чисел). Все это описывается такой формулой: $Ax = \lambda x$. В этом случае $\lambda$ является собственным значением матрицы $A$ (Eigenvalue), а $x$ - собственным вектором (Eigenvector).</description>
    </item>
    
  </channel>
</rss>
